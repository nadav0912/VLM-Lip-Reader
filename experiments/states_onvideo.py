import json
import cv2
import numpy as np
import os
from pathlib import Path
from datetime import timedelta
from dotenv import load_dotenv

load_dotenv()

import os

input_video_dir = os.getenv("RAW_VIDEOS_DIR", "")
input_transcript_dir = os.getenv("ROW_TRANSCRIPTS_DIR", "") 
print(input_video_dir)
print(input_transcript_dir)

VIDEO_PATH = os.path.join(input_video_dir, "Iman_Gadzhi_7_Principles_For_Teenagers_To_Become_Millionaires.mp4")
JSON_PATH = os.path.join(input_transcript_dir, "Iman_Gadzhi_7_Principles_For_Teenagers_To_Become_Millionaires.json")


def analyze_dataset(video_path, json_path):
    video_path = Path(video_path)
    json_path = Path(json_path)
    # 1. ×‘×“×™×§×ª ×§×™×•× ×§×‘×¦×™×
    if not video_path.exists() or not json_path.exists():
        print("Error: One or more files not found.")
        return

    # 2. ×˜×¢×™× ×ª × ×ª×•× ×™ ×”×•×™×“××•
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration_sec = total_frames / fps if fps > 0 else 0
    cap.release()

    # 3. ×˜×¢×™× ×ª × ×ª×•× ×™ ×”-JSON
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # × ×©×ª××© ×‘-word_segments ×›×™ ×–×” ×”× ×ª×•×Ÿ ×”××“×•×™×§ ×‘×™×•×ª×¨
    words = data.get('words', [])
    
    if not words:
        print("Error: No 'word_segments' found in JSON.")
        return

    # 4. ×¢×™×‘×•×“ ×•×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª
    word_durations = []  # ××©×š ×–××Ÿ ×‘×©× ×™×•×ª
    word_frames = []     # ××©×š ×–××Ÿ ×‘×¤×¨×™×™××™×
    gaps = []            # ×©×ª×™×§×•×ª ×‘×™×Ÿ ××™×œ×™×
    scores = []          # ×¨××ª ×‘×™×˜×—×•×Ÿ ×©×œ ×”××•×“×œ
    speaker_stats = {}   # ×¡×˜×˜×™×¡×˜×™×§×” ×œ×›×œ ×“×•×‘×¨

    prev_end = 0.0

    for w in words:
        start = w['start']
        end = w['end']
        duration = end - start
        
        # ×—×™×©×•×‘ ×¤×¨×™×™××™× (××¢×’×œ×™× ×œ××¡×¤×¨ ×”×©×œ× ×”×§×¨×•×‘)
        frames = (duration * fps)
        
        word_durations.append(duration)
        word_frames.append(frames)
        scores.append(w.get('score', 0))

        # ×—×™×©×•×‘ ×”×¤×¢×¨ ××”××™×œ×” ×”×§×•×“××ª (×©×ª×™×§×”)
        if prev_end > 0:
            gap = start - prev_end
            if gap > 0: # ××ª×¢×œ××™× ××—×¤×™×¤×•×ª ×§×˜× ×•×ª ×× ×™×©
                gaps.append(gap)
        prev_end = end

        # ×¡×˜×˜×™×¡×˜×™×§×ª ×“×•×‘×¨×™×
        spk = w.get('speaker', 'Unknown')
        speaker_stats[spk] = speaker_stats.get(spk, 0) + 1

    # ×”××¨×” ×œ-numpy arrays ×œ×—×™×©×•×‘×™× ××”×™×¨×™×
    np_durations = np.array(word_durations)
    np_frames = np.array(word_frames)
    np_scores = np.array(scores)
    np_gaps = np.array(gaps)

    # --- ×”×“×¤×¡×ª ×”×“×•×— ---
    print("="*60)
    print(f"ğŸ¬ VIDEO ANALYSIS REPORT: {os.path.basename(video_path)}")
    print("="*60)
    
    print(f"Video Stats:")
    print(f"  â€¢ Resolution:   {width}x{height}")
    print(f"  â€¢ FPS:          {fps:.2f}")
    print(f"  â€¢ Total Frames: {total_frames}")
    print(f"  â€¢ Duration:     {timedelta(seconds=int(duration_sec))}")

    print("-" * 60)
    print(f"ğŸ“ TEXT/WORD STATS (Total Words: {len(words)})")
    
    # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×–××Ÿ (×©× ×™×•×ª)
    print(f"\nTime per Word (Seconds):")
    print(f"  â€¢ Mean (Average):   {np.mean(np_durations):.4f} sec")
    print(f"  â€¢ Std Dev (Sigma):  {np.std(np_durations):.4f} sec")
    print(f"  â€¢ Median:           {np.median(np_durations):.4f} sec")
    print(f"  â€¢ Min:              {np.min(np_durations):.4f} sec")
    print(f"  â€¢ Max:              {np.max(np_durations):.4f} sec")

    # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¤×¨×™×™××™×
    print(f"\nFrames per Word (at {fps} FPS):")
    print(f"  â€¢ Mean (Average):   {np.mean(np_frames):.2f} frames")
    print(f"  â€¢ Std Dev (Sigma):  {np.std(np_frames):.2f} frames")
    print(f"  â€¢ Median:           {np.median(np_frames):.1f} frames")
    print(f"  â€¢ Min:              {np.min(np_frames)} frames")
    print(f"  â€¢ Max:              {np.max(np_frames)} frames")

    print(f"\nConfidence Scores (Model Certainty):")
    print(f"  â€¢ Average Score:    {np.mean(np_scores):.2%}")
    print(f"  â€¢ Lowest Score:     {np.min(np_scores):.2%}")

    if len(gaps) > 0:
        print(f"\nSilence/Gaps between words:")
        print(f"  â€¢ Average Gap:      {np.mean(np_gaps):.4f} sec")
        print(f"  â€¢ Max Gap:          {np.max(np_gaps):.4f} sec")

    print("-" * 60)
    print("ğŸ—£ï¸  SPEAKER DISTRIBUTION")
    for spk, count in speaker_stats.items():
        percentage = (count / len(words)) * 100
        print(f"  â€¢ {spk}: {count} words ({percentage:.1f}%)")

    print("-" * 60)
    print("âš ï¸  OUTLIERS & WARNINGS")
    
    # ×‘×“×™×§×ª ××™×œ×™× ×§×¦×¨×•×ª ××“×™ (×¤×—×•×ª ×-3 ×¤×¨×™×™××™× ×–×” ×‘×¢×™×™×ª×™ ×œ××•×“×œ)
    short_words_idx = np.where(np_frames < 3)[0]
    print(f"  â€¢ Extremely short words (<3 frames): {len(short_words_idx)}")
    if len(short_words_idx) > 0:
        print(f"    Examples: {[words[i]['word'] for i in short_words_idx[:5]]}")

    # ×‘×“×™×§×ª ××™×œ×™× ××¨×•×›×•×ª ××“×™ (××•×œ×™ ×©×’×™××ª ×¡× ×›×¨×•×Ÿ)
    long_words_idx = np.where(np_durations > 1.5)[0] # ××™×œ×” ××¢×œ 1.5 ×©× ×™×•×ª
    print(f"  â€¢ Very long words (>1.5 sec): {len(long_words_idx)}")
    if len(long_words_idx) > 0:
        print(f"    Examples: {[words[i]['word'] for i in long_words_idx[:5]]}")

    print("="*60)

if __name__ == "__main__":
    analyze_dataset(VIDEO_PATH, JSON_PATH)